{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RS_VAE_Modified.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"rtlyYedDuQa-"},"source":["import numpy as np\n","from tqdm import tqdm\n","import math\n","import tensorflow.compat.v1 as tf\n","tf.disable_eager_execution()\n","import argparse\n","import pandas as pd\n","from tensorflow.compat.v1.distributions import Bernoulli\n","from tensorflow.compat.v1.train import RMSPropOptimizer\n","import matplotlib.pyplot as plt\n","from scipy.sparse import save_npz, load_npz\n","from scipy.sparse import vstack, hstack"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nRPYdbgdp2p"},"source":["#load and save files using the file name and path specified\n","def load_numpy(path, name):\n","    return load_npz(path+name).tocsr()\n","\n","def load_dataframe_csv(path, name):\n","    return pd.read_csv(path+name)\n","    \n","def save_dataframe_csv(df, path, name):\n","    df.to_csv(path+name, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6jnk8O98zeV"},"source":["# VAE-CF Model\r\n"]},{"cell_type":"code","metadata":{"id":"Ct3d_BVaeaLt"},"source":["#make predictions based on the embbeding matrices\n","def predict(matrix_U, matrix_V, topK, matrix_Train, bias, item_based=False):\n","    bias_vector = bias.copy()\n","    prediction = []\n","\n","    for user_index in tqdm(range(matrix_U.shape[0])):\n","        vector_u = matrix_U[user_index]\n","        vector_train = matrix_Train[user_index]\n","        if item_based:   #user based, get single bias everytime for each user \n","            bias = bias_vector[user_index]\n","        \n","        if len(vector_train.nonzero()[0]) > 0:\n","            train_index = vector_train.nonzero()[1]\n","            predictions = matrix_V.dot(vector_u)\n","            if bias is not None:\n","                vector_predict = predictions + bias\n","\n","        else:\n","            vector_predict = np.zeros(matrix_Train.shape[1], dtype=np.float32)\n","\n","        prediction.append(vector_predict)\n","\n","    return np.vstack(prediction)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgPUTCsMfpGK"},"source":["#metrics\n","# Recall @ R\n","def recallk(vector_true_dense, hits):\n","    hits = len(hits.nonzero()[0])\n","    return float(hits)/len(vector_true_dense)\n","\n","#support function for ndcg@k\n","def dcg_at_k(r, k, method=0):\n","    r = np.asfarray(r)[:k]\n","    if r.size:\n","        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n","    return 0.\n","\n","# ndcg@k\n","def ndcg_at_k(hits, k=100, **unused):\n","    \"\"\"\n","    r: Relevance scores (list or numpy) in rank order\n","    \"\"\"\n","    dcg_max = dcg_at_k(sorted(hits, reverse=True), k)\n","    if not dcg_max:\n","        return 0.\n","    return dcg_at_k(hits, k) / dcg_max"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNGQ7cBk_HLr"},"source":["# evaluate the two metrics\r\n","def evaluate(matrix_Predict, matrix_Test, atK, metric_names= ['Recall', 'NDCG@K']):\r\n","    #mapping of metrics to functions\r\n","    local_metrics = {\r\n","        \"Recall\": recallk,\r\n","        \"NDCG@K\": ndcg_at_k\r\n","    }\r\n","\r\n","    output = {}\r\n","\r\n","    for k in atK:\r\n","        results = {name: [] for name in metric_names}\r\n","        #retrieve top k predictions\r\n","        topK_Predictions = matrix_Predict[:, :k]\r\n","\r\n","        for user_index in range(topK_Predictions.shape[0]):\r\n","            predicted = topK_Predictions[user_index]\r\n","            if len(predicted.nonzero()[0]) > 0:\r\n","                vector_true = matrix_Test[user_index]\r\n","                vector_dense = vector_true.nonzero()[1]\r\n","                hits = np.isin(predicted, vector_dense)\r\n","\r\n","                if vector_dense.size > 0:\r\n","                    for name in metric_names:\r\n","                        results[name].append(local_metrics[name](vector_true_dense=vector_dense, hits=hits))\r\n","        #create a summary of results\r\n","        summary = {}\r\n","        for name in metric_names:\r\n","            summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),1.96*np.std(results[name])/np.sqrt(matrix_Predict.shape[0]))\r\n","        output.update(summary)\r\n","\r\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GChEB4mMhEnF"},"source":["class VAE(object):\n","\n","    def __init__(self, observation_dim, latent_dim, batch_size,\n","                 lamb=0.01,optimizer=RMSPropOptimizer):\n","\n","        self._lamb = lamb\n","        self._beta = 0.2\n","        self._latent_dim = latent_dim\n","        self._batch_size = batch_size\n","        self._observation_dim = observation_dim\n","        print('observation dimension:', self._observation_dim)\n","        self._optimizer = optimizer\n","        self._observation_distribution = \"Multinomial\"\n","        self._observation_std = 0.01\n","        self._build_graph()\n","\n","    def _build_graph(self):\n","\n","        with tf.variable_scope('vae'):\n","            # create input as a column vector with the dimension of observations (item for user-based)\n","            self.input = tf.placeholder(tf.float32, shape=[None, self._observation_dim])\n","            # dropout rate of x are set to 0. The remaining elements are scaled up by 1.0 / (1 - rate)\n","            self.corruption = tf.placeholder(tf.float32)    # some corruption number\n","            self.sampling = tf.placeholder(tf.bool)         \n","            \n","            # adding a dropout layer \n","            mask1 = tf.nn.dropout(tf.ones_like(self.input), 1 - self.corruption)\n","\n","            input_dropout = self.input * mask1\n","\n","            with tf.variable_scope('encoder'):\n","                # encoded weight for mean and vairance with I * 2K or U * 2K (user vs. item based)\n","                # each K for e\n","                encode_weights = tf.Variable(tf.truncated_normal([self._observation_dim, self._latent_dim*2],\n","                                                                 stddev=1 / 500.0), name=\"Weights\")    ### !Hard-coded stdev! \n","                # encoded biases, diagonal \n","                encode_bias = tf.Variable(tf.constant(0., shape=[self._latent_dim*2]), name=\"Bias\")\n","\n","                # U * I @ I * 2K + ,2K  into the encoded space \n","                encoded = tf.matmul(input_dropout, encode_weights) + encode_bias\n","\n","            # gets the embedded z vector \n","            with tf.variable_scope('latent'):\n","                # get encoded mean \n","                self.mean = tf.nn.relu(encoded[:, :self._latent_dim])\n","                 # get standard deviation for latent \n","                logstd = encoded[:, self._latent_dim:]\n","                self.stddev = tf.exp(logstd)\n","                # for parameterization trick \n","                epsilon = tf.random_normal(tf.shape(self.stddev))\n","                \n","                self.z = tf.cond(self.sampling, lambda: self.mean + self.stddev * epsilon, lambda: self.mean)\n","\n","            with tf.variable_scope('decoder'):\n","\n","                self.decode_weights = tf.Variable(\n","                    tf.truncated_normal([self._latent_dim, self._observation_dim], stddev=1 / 500.0),\n","                    name=\"Weights\")\n","                self.decode_bias = tf.Variable(tf.constant(0., shape=[self._observation_dim]), name=\"Bias\")\n","                decoded = tf.matmul(self.z, self.decode_weights) + self.decode_bias\n","\n","                self.obs_mean = decoded\n","\n","                self.sample = Bernoulli(probs=self.obs_mean).sample()\n","\n","            # identification of loss functions \n","            with tf.variable_scope('loss'):\n","                # KL divergence\n","                with tf.variable_scope('kl-divergence'):\n","                    kl = self._kl_diagnormal_stdnormal(self.mean, logstd)\n","\n","                # multinomial log likelihood \n","                with tf.variable_scope('multinomial'):\n","                        obj = self._multinomial_log_likelihood(self.input, self.obs_mean)\n","                # l2 regularization \n","                with tf.variable_scope('l2'):\n","                    l2_loss = tf.reduce_mean(tf.nn.l2_loss(encode_weights) + tf.nn.l2_loss(self.decode_weights))\n","\n","                self._loss = self._beta * kl + obj + self._lamb * l2_loss\n","\n","            with tf.variable_scope('optimizer'):\n","                optimizer = self._optimizer(learning_rate=1e-4)\n","            with tf.variable_scope('training-step'):\n","                self._train = optimizer.minimize(self._loss)\n","\n","            self.sess = tf.Session()\n","            init = tf.global_variables_initializer()\n","            self.sess.run(init)\n","\n","    @staticmethod\n","    def _kl_diagnormal_stdnormal(mu, log_std):\n","        var_square = tf.exp(2 * log_std)\n","        return 0.5 * tf.reduce_mean(tf.square(mu) + var_square - 1. - 2 * log_std)\n","\n","    @staticmethod\n","    def _multinomial_log_likelihood(target, outputs, eps=1e-8):\n","        log_softmax_output = tf.nn.log_softmax(outputs)\n","        log_like = -tf.reduce_mean(tf.reduce_sum(log_softmax_output * target, axis=1))\n","        return log_like\n","\n","    # take UI if user-based, or else IU \n","    def train_model(self, rating_matrix, corruption, epoch=100):\n","        batches = self.get_batches(rating_matrix, self._batch_size)\n","\n","        # Training\n","        for i in tqdm(range(epoch)):\n","            for step in range(len(batches)):\n","                feed_dict = {self.input: batches[step].todense(), self.corruption: corruption, self.sampling: True}\n","                training = self.sess.run([self._train], feed_dict=feed_dict)\n","\n","    def get_batches(self, rating_matrix, batch_size):\n","        remaining_size = rating_matrix.shape[0]\n","        batch_index = 0\n","        batches = []\n","        while remaining_size > 0:\n","            if remaining_size < batch_size:\n","                batches.append(rating_matrix[batch_index*batch_size:])\n","            else:\n","                batches.append(rating_matrix[batch_index*batch_size:(batch_index+1)*batch_size])\n","            batch_index += 1\n","            remaining_size -= batch_size\n","        return batches\n","\n","    # get embeddings\n","    def get_RQ(self, rating_matrix):\n","        batches = self.get_batches(rating_matrix, self._batch_size)\n","        RQ = []\n","        for step in range(len(batches)):\n","            feed_dict = {self.input: batches[step].todense(), self.corruption: 0, self.sampling: False}\n","            embedding = self.sess.run(self.z, feed_dict=feed_dict)\n","            RQ.append(embedding)\n","\n","        return np.vstack(RQ)\n","\n","    def get_Y(self):\n","        return self.sess.run(self.decode_weights)\n","\n","    def get_Bias(self):\n","        return self.sess.run(self.decode_bias)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVDjiCpNNHwd"},"source":["def vae_cf(matrix_train, iteration=100, lam=80, rank=200, corruption=0.5):\r\n","    # record number of items for user-based, else number of users as _observation_dim\r\n","    m, n = matrix_train.shape\r\n","\r\n","    #we assume observation are sampled from multinomial distribution \r\n","    model = VAE(n, rank, 100, lamb=lam, optimizer=RMSPropOptimizer) # default batch_size as 100 here, \r\n","\r\n","    #train model\r\n","    model.train_model(matrix_train, corruption, iteration)\r\n","\r\n","    #get model parameters to be returned\r\n","    RQ = model.get_RQ(matrix_train)\r\n","    Y = model.get_Y()\r\n","    Bias = model.get_Bias()\r\n","\r\n","    #close tf session to avoid training more models on the same graph\r\n","    model.sess.close()\r\n","    tf.reset_default_graph()\r\n","\r\n","    return RQ, Y, Bias"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr1PwZQ2SFUb"},"source":["def main(args):\r\n","    # Show hyper parameter settings\r\n","    print(\"\\n___________parameter settings____________\")\r\n","    print(\"Data Path: {0}\".format(args.path))\r\n","    print(\"Rank: {0}\".format(args.rank))\r\n","    print(\"Lambda: {0}\".format(args.lamb))\r\n","    print(\"Number of iterations: {0}\".format(args.iter))\r\n","    print(\"Evaluation - Topk: {0}\".format(args.topk))\r\n","\r\n","    # Load Data\r\n","    print(\"\\n___________load data____________\")\r\n","    R_train = load_numpy(path=args.path, name='Rtrain.npz')\r\n","    R_valid = load_numpy(path=args.path, name='Rvalid.npz')\r\n","    R_train = R_train + R_valid\r\n","    print(\"Train U-I Dimensions: {0}\".format(R_train.shape))\r\n","\r\n","    \"\"\"User-based entry:\"\"\"\r\n","    print(\"\\n___________User-based VAE-CF computation:____________\")\r\n","    # RQ has dimension U * K\r\n","    # Y has dimension I * K\r\n","    # Bias has dimension (I,)\r\n","    RQ, Yt, Bias = vae_cf(R_train, iteration=args.iter, rank=args.rank,\r\n","                                      corruption=args.corruption,\r\n","                                      lam=args.lamb)\r\n","    Y = Yt.T\r\n","    \r\n","    print('RQ shape:', RQ.shape)\r\n","    print('Y shape', Y.shape)\r\n","    print('Bias shape:', Bias.shape)\r\n","\r\n","    print(\"\\n___________predict for user-based VAE:____________\")\r\n","    user_based_prediction = predict(matrix_U=RQ,\r\n","                            matrix_V=Y,\r\n","                            bias=Bias,\r\n","                            topK=args.topk,\r\n","                            matrix_Train=R_train,\r\n","                            item_based = False)\r\n","    \r\n","    print(user_based_prediction.shape)\r\n","\r\n","    \"\"\"Item-based entry\"\"\"\r\n","    print(\"\\n___________Item-based VAE-CF computation:____________\")\r\n","    # Y has dimension I * K\r\n","    # RQ has dimension U * K\r\n","    # Bias has dimension (U,)\r\n","    Y, RQt, Bias = vae_cf(R_train.T, iteration=args.iter, rank=args.rank,\r\n","                                      corruption=args.corruption,\r\n","                                      lam=args.lamb)\r\n","    RQ = RQt.T\r\n","    \r\n","    print('RQ shape:', RQ.shape)\r\n","    print('Y shape', Y.shape)\r\n","    print('Bias shape:', Bias.shape)\r\n","\r\n","    print(\"\\n___________predict for item-based VAE:____________\")\r\n","    item_based_prediction = predict(matrix_U=RQ,\r\n","                            matrix_V=Y,\r\n","                            bias=Bias,\r\n","                            topK=args.topk,\r\n","                            matrix_Train=R_train,\r\n","                            item_based = True)\r\n","\r\n","    print(item_based_prediction.shape)\r\n","\r\n","    print(\"\\n___________weighted  user-item-based VAE:____________\")\r\n","    # combine user and item based predictions\r\n","    UI_user_100_normalized = (user_based_prediction - np.mean(user_based_prediction)) / np.std(user_based_prediction)\r\n","    UI_item_100_normalized = (item_based_prediction - np.mean(item_based_prediction)) / np.std(item_based_prediction)\r\n","    weighted_UI = args.weight_u * UI_user_100_normalized + args.weight_i * UI_item_100_normalized\r\n","\r\n","    UI_prediction = []\r\n","    # create top k predictions\r\n","    for user_index in tqdm(range(R_train.shape[0])):\r\n","        # get user training data \r\n","        vector_train = R_train[user_index]\r\n","        train_index = vector_train.nonzero()[1]\r\n","        vector_predict = weighted_UI[user_index]\r\n","\r\n","        # sort candidate items with descending predicted score \r\n","        candidate_index = np.argpartition(-vector_predict, \\\r\n","                                          args.topk+len(train_index))[:args.topk+len(train_index)]\r\n","\r\n","        # get only the topk items and delete the ones inside the training dataset\r\n","        vector_predict = candidate_index[vector_predict[candidate_index].argsort()[::-1]]\r\n","        vector_predict = np.delete(vector_predict, np.isin(vector_predict, train_index).nonzero()[0])\r\n","        UI_prediction.append(vector_predict[:args.topk])\r\n","    \r\n","    UI_prediction = np.array(UI_prediction)\r\n","    print(UI_prediction.shape)\r\n","\r\n","    print(\"\\n___________create metrics____________\")\r\n","\r\n","    metric_names = ['Recall', 'NDCG@K']\r\n","    R_valid = load_numpy(path=args.path, name='Rtest.npz')\r\n","    result = evaluate(UI_prediction, R_valid, metric_names, [args.topk])\r\n","    print(\"\\n\")\r\n","    for metric in result.keys():\r\n","        print(\"{0}:{1}\".format(metric, result[metric]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCYQkNiXdAbt"},"source":["def main(args):\n","    # Show hyper parameter settings\n","    print(\"\\n___________parameter settings____________\")\n","    print(\"Data Path: {0}\".format(args.path))\n","    print(\"Rank: {0}\".format(args.rank))\n","    print(\"Lambda: {0}\".format(args.lamb))\n","    print(\"Number of iterations: {0}\".format(args.iter))\n","    print(\"Evaluation - Topk: {0}\".format(args.topk))\n","\n","    # Load Data\n","    print(\"\\n___________load data____________\")\n","    R_train = load_numpy(path=args.path, name='Rtrain.npz')\n","    R_valid = load_numpy(path=args.path, name='Rvalid.npz')\n","    R_train = R_train + R_valid\n","    print(\"Train U-I Dimensions: {0}\".format(R_train.shape))\n","\n","    \"\"\"User-based entry:\"\"\"\n","    print(\"\\n___________User-based VAE-CF computation:____________\")\n","    # RQ has dimension U * K\n","    # Y has dimension I * K\n","    # Bias has dimension (I,)\n","    RQ, Yt, Bias = vae_cf(R_train, iteration=args.iter, rank=args.rank,\n","                                      corruption=args.corruption,\n","                                      lam=args.lamb)\n","    Y = Yt.T\n","    \n","    print('RQ shape:', RQ.shape)\n","    print('Y shape', Y.shape)\n","    print('Bias shape:', Bias.shape)\n","\n","    print(\"\\n___________predict for user-based VAE:____________\")\n","    user_based_prediction = predict(matrix_U=RQ,\n","                            matrix_V=Y,\n","                            bias=Bias,\n","                            topK=args.topk,\n","                            matrix_Train=R_train,\n","                            item_based = False)\n","    \n","    print(user_based_prediction.shape)\n","\n","    \"\"\"Item-based entry\"\"\"\n","    print(\"\\n___________Item-based VAE-CF computation:____________\")\n","    # Y has dimension I * K\n","    # RQ has dimension U * K\n","    # Bias has dimension (U,)\n","    Y, RQt, Bias = vae_cf(R_train.T, iteration=args.iter, rank=args.rank,\n","                                      corruption=args.corruption,\n","                                      lam=args.lamb)\n","    RQ = RQt.T\n","    \n","    print('RQ shape:', RQ.shape)\n","    print('Y shape', Y.shape)\n","    print('Bias shape:', Bias.shape)\n","\n","    print(\"\\n___________predict for item-based VAE:____________\")\n","    item_based_prediction = predict(matrix_U=RQ,\n","                            matrix_V=Y,\n","                            bias=Bias,\n","                            topK=args.topk,\n","                            matrix_Train=R_train,\n","                            item_based = True)\n","\n","    print(item_based_prediction.shape)\n","\n","    print(\"\\n___________weighted  user-item-based VAE:____________\")\n","    # combine user and item based predictions\n","    UI_user_100_normalized = (user_based_prediction - np.mean(user_based_prediction)) / np.std(user_based_prediction)\n","    UI_item_100_normalized = (item_based_prediction - np.mean(item_based_prediction)) / np.std(item_based_prediction)\n","    weighted_UI = args.weight_u * UI_user_100_normalized + args.weight_i * UI_item_100_normalized\n","\n","    UI_prediction = []\n","    # create top k predictions\n","    for user_index in tqdm(range(R_train.shape[0])):\n","        # get user training data \n","        vector_train = R_train[user_index]\n","        train_index = vector_train.nonzero()[1]\n","        vector_predict = weighted_UI[user_index]\n","\n","        # sort candidate items with descending predicted score \n","        candidate_index = np.argpartition(-vector_predict, \\\n","                                          args.topk+len(train_index))[:args.topk+len(train_index)]\n","\n","        # get only the topk items and delete the ones inside the training dataset\n","        vector_predict = candidate_index[vector_predict[candidate_index].argsort()[::-1]]\n","        vector_predict = np.delete(vector_predict, np.isin(vector_predict, train_index).nonzero()[0])\n","        UI_prediction.append(vector_predict[:args.topk])\n","    \n","    print(np.array(UI_prediction).shape)\n","\n","    print(\"\\n___________create metrics____________\")\n","    R_test = load_numpy(path=args.path, name='Rtest.npz')\n","    result = evaluate(np.array(UI_prediction), R_test, [args.topk])\n","    print(\"\\n\")\n","    for metric in result.keys():\n","        print(\"{0}:{1}\".format(metric, result[metric]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcvckIWRLyZi","executionInfo":{"status":"ok","timestamp":1607810028505,"user_tz":300,"elapsed":12542,"user":{"displayName":"Sophie Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMCCodhe0PPZJ42crtTIT5wXQCsr1LULZwyb9C=s64","userId":"06280324160309011021"}},"outputId":"6a4fad58-8920-4b48-b10b-b6bfa0023941"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eQjhOFmpL7Jb"},"source":["#Tina\r\n","#data_path = '/content/drive/MyDrive/MASc_First_Year/CSC2515/CSC2515_Project_Part2/data/npz_files_threshold_4/'\r\n","\r\n","#Sophie\r\n","data_path = '/content/drive/MyDrive/CSC2515_Project_Part2/data/npz_files_threshold_4/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRZecoc77m8e"},"source":["best_hyper = {'NDCG': '[0.245, 0.0111]',\n","              'Recall@K': '[0.3625, 0.0169]',\n","              'corruption': 0.2,\n","              'iter': 200,\n","              'lambda': 0.0001,\n","              'rank': 100,\n","              'tok': 50.0,\n","              'weight_i': 0.1,\n","              'weight_u': 0.9}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bC8XfO_dGSd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607817407046,"user_tz":300,"elapsed":31510,"user":{"displayName":"Sophie Tian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiMCCodhe0PPZJ42crtTIT5wXQCsr1LULZwyb9C=s64","userId":"06280324160309011021"}},"outputId":"8fff1b3b-e901-425f-8d46-01f9f8d27927"},"source":["#test run\n","if __name__ == \"__main__\":\n","    # Commandline arguments\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-i', dest='iter', type=int, default=200)\n","    parser.add_argument('-l', dest='lamb', type=float, default=0.0001)\n","    parser.add_argument('-r', dest='rank', type=int, default=100)         # latent dimension\n","    parser.add_argument('-f')\n","    parser.add_argument('-c', dest='corruption', type=float, default=0.2)    #default dropout rate\n","    parser.add_argument('-d', dest='path', default=data_path)\n","    parser.add_argument('-k', dest='topk', type=int, default=50)\n","    parser.add_argument('-weight_u', dest='weight_u', type=float, default=0.9)\n","    parser.add_argument('-weight_i', dest='weight_i', type=float, default=0.1)\n","    args = parser.parse_args()\n","\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","___________parameter settings____________\n","Data Path: /content/drive/MyDrive/CSC2515_Project_Part2/data/npz_files_threshold_4/\n","Rank: 100\n","Lambda: 0.0001\n","Number of iterations: 100\n","Evaluation - Topk: 50\n","\n","___________load data____________\n","Train U-I Dimensions: (943, 1682)\n","\n","___________User-based VAE-CF computation:____________\n","observation dimension: 1682\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [00:12<00:00,  7.80it/s]\n"," 18%|█▊        | 173/943 [00:00<00:00, 1634.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["RQ shape: (943, 100)\n","Y shape (1682, 100)\n","Bias shape: (1682,)\n","\n","___________predict for user-based VAE:____________\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 943/943 [00:00<00:00, 1800.15it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["(943, 1682)\n","\n","___________Item-based VAE-CF computation:____________\n","observation dimension: 943\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 100/100 [00:15<00:00,  6.57it/s]\n"," 21%|██        | 194/943 [00:00<00:00, 1932.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["RQ shape: (943, 100)\n","Y shape (1682, 100)\n","Bias shape: (943,)\n","\n","___________predict for item-based VAE:____________\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 943/943 [00:00<00:00, 1968.20it/s]\n"," 23%|██▎       | 215/943 [00:00<00:00, 2144.21it/s]"],"name":"stderr"},{"output_type":"stream","text":["(943, 1682)\n","\n","___________weighted  user-item-based VAE:____________\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 943/943 [00:00<00:00, 2589.16it/s]\n"," 40%|████      | 378/943 [00:00<00:00, 3775.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["(943, 50)\n","\n","___________create metrics____________\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 943/943 [00:00<00:00, 3836.89it/s]\n","100%|██████████| 943/943 [00:00<00:00, 1652.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Recall@50:(0.3269228351037726, 0.014849081519205541)\n","NDCG@K:(0.4724168546173931, 0.014894197793072874)\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"jIU1z589lbUo"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"TtJc3Plylawl"},"source":["def hyper_parameter_tuning(R_train, R_valid, params, save_path):\n","  # df = pd.read_csv(save_path+'hyperparam_continued.csv')\n","  df = pd.DataFrame(columns=list(params.keys()))\n","\n","  print(\"Train U-I Dimensions: {0}\".format(R_train.shape))\n","\n","  for rank in params['rank']:\n","    for lam in params['lambda']:\n","      for corruption in params['corruption']:\n","        for topK in params['topK']:\n","          for weight_u_i in params['weight_u_i']:\n","\n","            print(\"rank: {0}, lambda: {1}, corruption: {2}, topK: {3}, wegith_u_i: {4}\"\\\n","                              .format(rank, lam, corruption, topK, weight_u_i))\n","            \n","            RQ, Yt, Bias = vae_cf(R_train, iteration=params['iter'], rank=rank,\n","                                              corruption=corruption,\n","                                              lam=lam)\n","            Y = Yt.T\n","            user_based_prediction = predict(matrix_U=RQ,\n","                                    matrix_V=Y,\n","                                    bias=Bias,\n","                                    topK=topK,\n","                                    matrix_Train=R_train,\n","                                    item_based = False)\n","\n","            Y, RQt, Bias = vae_cf(R_train.T, \n","                                iteration=params['iter'], rank=rank,\n","                                corruption=corruption,lam=lam)\n","            RQ = RQt.T\n","            item_based_prediction = predict(matrix_U=RQ,\n","                                    matrix_V=Y, bias=Bias,\n","                                    topK=topK, matrix_Train=R_train,\n","                                    item_based = True)\n","\n","            # combine user and item based predictions\n","            UI_user_100_normalized = (user_based_prediction - np.mean(user_based_prediction)) / np.std(user_based_prediction)\n","            UI_item_100_normalized = (item_based_prediction - np.mean(item_based_prediction)) / np.std(item_based_prediction)\n","            weighted_UI = weight_u_i[0] * UI_user_100_normalized + weight_u_i[1] * UI_item_100_normalized\n","\n","            UI_prediction = []\n","            # create top k predictions\n","            for user_index in tqdm(range(R_train.shape[0])):\n","                # get user training data \n","                vector_train = R_train[user_index]\n","                train_index = vector_train.nonzero()[1]\n","                vector_predict = weighted_UI[user_index]\n","\n","                # sort candidate items with descending predicted score \n","                candidate_index = np.argpartition(-vector_predict, topK+len(train_index))[:topK+len(train_index)]\n","                # get only the topk items and delete the ones inside the training dataset\n","                vector_predict = candidate_index[vector_predict[candidate_index].argsort()[::-1]]\n","                vector_predict = np.delete(vector_predict, np.isin(vector_predict, train_index).nonzero()[0])\n","                UI_prediction.append(vector_predict[:topK])\n","            \n","            result = evaluate(np.array(UI_prediction), R_valid, params['metric'], [topK])\n","            \n","            result_dict = {'rank': rank, 'lambda': lam, 'topK': topK,\\\n","                            'iter': params['iter'], 'similarity': params['similarity'],\\\n","                            'corruption': corruption, 'weight_u': weight_u_i[0], \\\n","                            'weight_i': weight_u_i[1]}\n","\n","            # add in metric results\n","            for name in result.keys():\n","                result_dict[name] = [round(result[name][0], 4), round(result[name][1], 4)]\n","            df = df.append(result_dict, ignore_index=True)\n","            save_dataframe_csv(df, save_path, 'hyperparams_tunning.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_8uaK9xfq41S"},"source":["def main(args):\n","    params = {'corruption': [0.2, 0.3, 0.4, 0.5], \\\n","              'rank': [50, 100], 'topK': [50,100], \\\n","              'iter': 200, 'lambda': [0.00001, 0.0001, 0.001], \\\n","              'weight_u_i':[(1,0), (0.9,0.1),(0.8,0.2),(0.7,0.3),(0.6,0.4),(0.5,0.5),\\\n","                            (0.4,0.6), (0.3,0.7), (0.2,0.8),(0.1,0.9),(0,1)],\\\n","              'metric': ['NDCG@K', 'Recall']}\n","\n","    # load training and testing data \n","    R_train = load_numpy(path=args.path, name='Rtrain.npz')\n","    R_valid = load_numpy(path=args.path, name='Rvalid.npz')\n","    \n","    # merge training and validation data \n","    R_train = R_train + R_valid\n","    R_test = load_numpy(path=args.path, name='Rtest.npz')\n","\n","    # call hyperparameter tuning method\n","    hyper_parameter_tuning(R_train, R_test, params, save_path=args.hyper_path)\n","\n","if __name__ == \"__main__\":\n","    hyperparameter_folder = '/content/drive/MyDrive/MASc_First_Year/CSC2515/CSC2515_Project_Part2/hyperparameter_tuning_thresholded/'\n","\n","    # Commandline arguments\n","    parser = argparse.ArgumentParser(description=\"ParameterTuning\")\n","    parser.add_argument('-d', dest='path', default=data_path)\n","    parser.add_argument('-hyper_path', dest='hyper_path', default=hyperparameter_folder)\n","    parser.add_argument('-f')\n","    args = parser.parse_args()\n","\n","    main(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cT7MghBgR7Wv"},"source":["final_UI_prediction = np.load('results/final_UI100.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"miHxzKnuzf9P"},"source":["df = pd.read_csv('/content/drive/MyDrive/MASc_First_Year/CSC2515/CSC2515_Project_Part2/hyperparameter_tuning/hyperparam.csv')"],"execution_count":null,"outputs":[]}]}